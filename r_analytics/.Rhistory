### LOADING LIBRARY
#library("rjson") -- not working
library("jsonlite")
### LOADING LIBRARY
#library("rjson") -- not working
library("NLP")
install.packages("RWeka")
library("rjava")
install.packages("rjava")
install.packages("rJava")
install.packages("RWeka")
library("rJava")
install.packages("libjvm.dylib")
library("libjvm.dylib")
sudo R CMD javareconf
install.packages("rJava")
library("rJava")
library("jsonlite")
library("tm")
library("rJava")
library("wordcloud")
library("textir")
library("RWeka")
library("qdap")
library("maptpx")
######### BEGIN EXPLORE #########
articles <- fromJSON("../datasets/articles.json")
excerptCorpus<- iconv(articles$excerpt,to="utf-8-mac")
ecorpus<-Corpus(VectorSource(excerptCorpus))
ecorpus<-tm_map(ecorpus,tolower) #make it all lowercase
ecorpus<-tm_map(ecorpus,removePunctuation)
ecorpus<-tm_map(ecorpus,removeNumbers)
ecleaned<-tm_map(ecorpus,removeWords,stopwords("english"))
#remove united and airlines
ecleaned<-tm_map(ecleaned,removeWords,c("united","airlines","flight","airline","passenger","flights","will","according"))
ecleaned<-tm_map(ecleaned,stripWhitespace)
inspect(ecleaned[1:5])
#### TERM DOCUMENT MATRIX ####
#to structure the data
etdm<-TermDocumentMatrix(ecleaned)
etdm
ematrix<-as.matrix(etdm)
stopwords("en")
ematrix[1:10,1:20]
#BARPLOT
w <- rowSums(ematrix)
barplot(w)
w[1:10]
w <- w[w>1000]
barplot(w,las=2)
window()
window(0)
`window<-`()
new window()
typeof(w)
class(w)
w <- w[w>2000]
barplot(w,las=2)
ematrix["..."]
ematrix["_"]
ematrix
w
ematrix["—"]
w["—"]
#remove united and airlines
ecleaned<-tm_map(ecleaned,removeWords,c("united","airlines","flight","airline","passenger","flights","will","according","…","—"))
ecleaned<-tm_map(ecleaned,stripWhitespace)
inspect(ecleaned[1:5])
ematrix<-NULL
etdm<-NULL
#### TERM DOCUMENT MATRIX ####
#to structure the data
etdm<-TermDocumentMatrix(ecleaned)
etdm
ematrix<-as.matrix(etdm)
#BARPLOT
w <- rowSums(ematrix)
w[1:10]
ov2k <- w[w>2000]
barplot(ov2k,las=2)
barplot(ov2k,las=2)
barplot(ov2k,las=2,main="Over 2000")
w["…"]
ov2k <- w[w>1000]
barplot(ov2k,las=2,main="Over 2000")
ecorpus<-tm_map(ecorpus,removePunctuation)
ecorpus<-tm_map(ecorpus,tolower) #make it all lowercase
ecorpus<-tm_map(ecorpus,removePunctuation)
ecorpus<-tm_map(ecorpus,removeNumbers)
ecleaned<-tm_map(ecorpus,removeWords,stopwords("english"))
#remove united and airlines
ecleaned<-tm_map(ecleaned,removeWords,c("united","airlines","flight","airline","passenger","flights","will","according","…","—"))
inspect(ecleaned[1:5])
ecleaned<-tm_map(ecleaned,stripWhitespace)
etdm<-NULL
ematrix<-NULL
#### TERM DOCUMENT MATRIX ####
#to structure the data
etdm<-TermDocumentMatrix(ecleaned)
etdm
ematrix<-as.matrix(etdm)
#BARPLOT
w <- rowSums(ematrix)
w[1:10]
ov2k <- w[w>1000]
barplot(ov2k,las=2,main="Over 2000")
w["…"]
w["'s"]
w
ov2k
excerptCorpus<- iconv(articles$excerpt,to="utf-8-mac")
ecorpus<-Corpus(VectorSource(excerptCorpus))
ecorpus<-tm_map(ecorpus,tolower) #make it all lowercase
ecorpus<-tm_map(ecorpus,removePunctuation)
ecorpus<-tm_map(ecorpus,removeNumbers)
ecleaned<-tm_map(ecorpus,removeWords,stopwords("english"))
#remove united and airlines
ecleaned<-tm_map(ecleaned,removeWords,c("united","airlines","flight","airline","passenger","flights","will","according","…","—","’s"))
ecleaned<-tm_map(ecleaned,stripWhitespace)
ematrix<-NULL
#### TERM DOCUMENT MATRIX ####
#to structure the data
etdm<-TermDocumentMatrix(ecleaned)
ematrix<-as.matrix(etdm)
w["…"]
#BARPLOT
w <- rowSums(ematrix)
sw<-sort(w)
w["…"]
w["’s"]
sw
sw<-sort(w,TRUE)
sw
w["new"]
sw["new"]
sw[1:100]
barplot(sw[1:100],las=2,main="Over 2000")
barplot(sw[1:1000],las=2,main="Over 2000")
#WORDCLOUD
?set.seed
wordcloud(words=names(sw),
freq=sw)
wordcloud(words=names(sw),
freq=sw,
max.words = 200,
min.freq = 200,
colors = brewer.pal(8,"Dark2"))
#remove united and airlines
ecleaned<-tm_map(ecleaned,removeWords,c("united","airlines","flight","airline","passenger","flights","will","according","…","—","’s","says","said"))
ecleaned<-tm_map(ecleaned,stripWhitespace)
#### TERM DOCUMENT MATRIX ####
#to structure the data
etdm<-TermDocumentMatrix(ecleaned)
ematrix<-NULL
ematrix<-as.matrix(etdm)
#BARPLOT
w <- rowSums(ematrix)
sw<-sort(w,TRUE)
wordcloud(words=names(sw),
freq=sw,
max.words = 200,
min.freq = 200,
colors = brewer.pal(8,"Dark2"))
#WORDCLOUD
set.seed(222)
wordcloud(words=names(sw),
freq=sw,
max.words = 200,
min.freq = 200,
colors = brewer.pal(8,"Dark2"))
ematrix<-NULL
install.packages("syuzhet")
library("syuzhet")
#### SENTIMENT ANALYSIS ####
get_nrc_sentiment(etdm)
#### SENTIMENT ANALYSIS ####
get_nrc_sentiment(ecleaned)
ecleaned
attributes(ecleaned)
df <- data.frame(text = get("content", ecleaned))
head(df)
sentiment <- get_nrc_sentiment(df$text)
df$text
head(df$text)
str(df)
df <- data.frame(text = get("content", ecleaned),stringsAsFactors = F)
head(df$text)
sentiment <- get_nrc_sentiment(df$text)
barplot(table(articles$sentiment,articles$source$country),las=2)
stopwords("en")
etdm
ematrix<-as.matrix(etdm)
ematrix[1:10,1:20]
barplot(sw[1:1000],las=2,main="Over 2000")
wordcloud(words=names(sw),
freq=sw,
max.words = 200,
min.freq = 200,
colors = brewer.pal(8,"Dark2"))
df$s<-sentiment
head(df$s)
#### EXPORTING CSV ####
selected <- articles[-c("has_video","topics")]
#### EXPORTING CSV ####
selected <- articles[select=-c("has_video","topics")]
#### EXPORTING CSV ####
selected <- subset(articles,select=-c("has_video","topics"))
#### EXPORTING CSV ####
selected <- articles[,!names(articles) %in% c("has_video","topics")]
?sample
first <- sample(1:39109,9777)
k1<-selected[first]
k1<-selected[first,]
selected<-selected[-first,]
second <- sample(1:29332,9777)
k2<-selected[second,]
selected<-selected[-second,]
second <- sample(1:19555,9777)
third <- sample(1:19555,9777)
k3<-selected[third,]
k4<-selected[-third,]
write.csv(k1,row.names = F,file="../four-fold-export/k1.csv")
write.csv(k1,row.names = F,"../four-fold-export/k1.csv")
write.csv(k1,"../four-fold-export/k1.csv",row.names = F)
?write.csv
head(k1)
write.csv(k1,"../four-fold-export/k1.csv",row.names = F)
View(k1)
?flatten
k1<-flatten(k1)
k2<flatten(k2)
k3<flatten(k3)
k2<-flatten(k2)
k3<-flatten(k3)
k4<flatten(k4)
k4<-flatten(k4)
write.csv(k1,"../four-fold-export/k1.csv",row.names = F)
View(k1)
#### EXPORTING CSV ####
selected <- articles[,!names(articles) %in% c("has_video","topics","image_link","link","source.link")]
first <- sample(1:39109,9777)
k1<-selected[first,]
k1<-flatten(k1)
selected<-selected[-first,]
selected<-flatten(selected)
first <- sample(1:39109,9777)
k1<-selected[first,]
selected<-selected[-first,]
second <- sample(1:29332,9777)
k2<-selected[second,]
selected<-selected[-second,]
third <- sample(1:19555,9777)
k3<-selected[third,]
k4<-selected[-third,]
write.csv(k1,"../four-fold-export/k1.csv",row.names = F)
selected$authors <- c(selected$authors)
write.csv(unlist(k1),"../four-fold-export/k1.csv",row.names = F)
write.csv(unlist(k2),"../four-fold-export/k2.csv",row.names = F)
write.csv(unlist(k3),"../four-fold-export/k3.csv",row.names = F)
write.csv(unlist(k4),"../four-fold-export/k4.csv",row.names = F)
test1<-read.csv("../four-fold-export/k1.csv")
head(test1)
k1$authors<-unlist(k1$authors)
write.csv(k1,"../four-fold-export/k1.csv",row.names = F)
View(k1)
k1$authors<-unlist(k1$authors)
?unlist
k1$authors[where(k1$authors==NULL)]<-"NA"
k1$authors[k1$authors==NULL]<-"NA"
k1$authors[is.null(k1$authors)]<-"NA"
is.null(k1$authors)
k1$authors[where(is.null(k1$authors))]<-"NA"
#### EXPORTING CSV ####
selected <- articles[,!names(articles) %in% c("authors","has_video","topics","image_link","link","source.link")]
selected$authors <- c(selected$authors)
first <- sample(1:39109,9777)
#### EXPORTING CSV ####
selected <- articles[,!names(articles) %in% c("authors","has_video","topics","image_link","link","source.link")]
selected <- flatten(selected)
first <- sample(1:39109,9777)
k1<-selected[first,]
selected<-selected[-first,]
second <- sample(1:29332,9777)
k2<-selected[second,]
selected<-selected[-second,]
third <- sample(1:19555,9777)
k3<-selected[third,]
k4<-selected[-third,]
write.csv(k1,"../four-fold-export/k1.csv",row.names = F)
write.csv(k2,"../four-fold-export/k2.csv",row.names = F)
write.csv(k3,"../four-fold-export/k3.csv",row.names = F)
write.csv(k4,"../four-fold-export/k4.csv",row.names = F)
test1<-read.csv("../four-fold-export/k1.csv")
head(test1)
tpub<-table(articles$source$publisher)
tpub<-sort(tpub)
?sort
tpub<-sort(tpub,TRUE)
tpub[1:10]
barplot(tpub[1:10])
barplot(tpub[1:10],las=2)
window()
window()
tapply(tpub, FUN=gsub("\.com",""))
gsub("\.com","",tpub)
gsub(".com","",tpub)
tpub[1:10]
gsub(".com","",tpub[1:10])
barplot(tpub[1:10],las=2,col="red")
barplot(tpub[1:10],las=2,
main="Top 10 Publishing Sources",
ylab="Amount of Articles",
xlab="Sources",
col="red")
gsub(".com","",tpub[tpub[1:10]])
barplot(tpub[1:10],las=2,
main="Top 10 Publishing Sources",
ylab="Amount of Articles",
xlab="Sources",
names.arg = c("yahoo","cbsnews","boardingarea","nbc","foxnews","flyertalk","aviationpros","4-traders.com","sfgate","thevillagesuntimes"),
col="red")
grid()
barplot(tpub[1:10],las=1,
main="Top 10 Publishing Sources",
ylab="Amount of Articles",
xlab="Sources",
names.arg = c("yahoo","cbsnews","boardingarea","nbc","foxnews","flyertalk","aviationpros","4-traders.com","sfgate","thevillagesuntimes"),
col="red")
#head(test1)
tpub <- table(articles$source$publisher)
tpub<- sort(tpub,T)
install.packages("topicmodels")
library("topicmodels")
######### BEGIN EXPLORE #########
articles <- fromJSON("../datasets/articles.json")
######### LOADING LIBRARY #########
#library("rjson") -- not working
library("jsonlite")
library("tm")
#library("rJava")
library("wordcloud")
#library("textir")
#library("RWeka")
#library("qdap")
#library("maptpx")
library("syuzhet")
library("topicmodels")
######### BEGIN EXPLORE #########
articles <- fromJSON("../datasets/articles.json")
excerptCorpus<- iconv(articles$excerpt,to="utf-8-mac")
ecorpus<-Corpus(VectorSource(excerptCorpus))
removeURL<- function(x) gsub('http[[:alnum:]]*','',x)
ecorpus<-tm_map(ecorpus,tolower) #make it all lowercase
ecorpus<-tm_map(ecorpus,removePunctuation)
ecorpus<-tm_map(ecorpus,removeNumbers)
stopwords("english")
ecleaned<-tm_map(ecorpus,removeWords,stopwords("english"))
#remove united and airlines
ecleaned<-tm_map(ecleaned,removeWords,c("united","airlines","flight","airline","passenger","flights","will","according","…","—","’s","says","said"))
ecleaned<-tm_map(ecleaned,stripWhitespace)
inspect(ecleaned[1:5])
#### TERM DOCUMENT MATRIX ####
#to structure the data
etdm<-TermDocumentMatrix(ecleaned)
# set a seed so that the output of the model is predictable
etdm_lda <- LDA(etdm, k = 2, control = list(seed = 1234))
etdm_lda
library("tidytext")
install.packages("tidytext")
library("tidytext")
etdm_topics <- tidy(etdm_lda,matrix="beta")
etdm_topics
etdm_topics <- tidy(etdm_lda)
etdm_topics
etdm_topics <- tidy(etdm_lda,matrix="beta")[100:150]
etdm_topics[100:150]
etdm_topics[100:150,]
etdm
#### TOPIC CLUSTERING ####
tf<-TermDocFreq(dtm=etdm)
topics(etdm_lda)
topics(etdm_lda)[1:10]
terms(etdm_lda)[1:10]
as.matrix(topics(etdm_lda)[1:10])
as.matrix(terms(etdm_lda,6))
etdm_topics <- tidy(topics(etdm_lda),matrix="beta")
as.matrix(topics(etdm_lda,6))
as.matrix(topics(etdm_lda)[1:10])
ldamat<-as.matrix(topics(etdm_lda))
v1<-which(ldamat[,1]==1)
v2<-which(ldamat[,1]==2)
v1
# set a seed so that the output of the model is predictable
topics =4
# set a seed so that the output of the model is predictable
k =4
etdm_lda <- LDA(etdm, k = k, control = list(seed = 1234))
# set a seed so that the output of the model is predictable
k =4
etdm_lda <- LDA(etdm, k = k, control = list(seed = 1234))
View(articles)
View(articles)
ldamat<-as.matrix(topics(etdm_lda))
for (v in seq(1:k)) {
which(ldamat[,1]==v)[1:20]
}
for (v in seq(1:k)) {
l<-which(ldamat[,1]==v)[1:20]
l
}
l<-c()
l<-c()
for (v in seq(1:k)) {
l<-c(l,which(ldamat[,1]==v)[1:20])
}
l
l<-data.frame()
for (v in seq(1:k)) {
l$v<-which(ldamat[,1]==v)[1:20]
}
for (v in seq(1:k)) {
l$`v`<-which(ldamat[,1]==v)[1:20]
}
l<-data.frame(-1=seq(1:20))
l<-data.frame(`-1`=seq(1:20))
for (v in seq(1:k)) {
l$`v`<-which(ldamat[,1]==v)[1:20]
}
l
l<-data.frame(`-1`=seq(1:20))
for (v in seq(1:k)) {
l$v<-which(ldamat[,1]==v)[1:20]
}
l
l<-data.frame(`-1`=seq(1:20))
for (v in seq(1:k)) {
l$v<-ldamat[which(ldamat[,1]==v)][1:20]
}
l
n<-seq(1:k)
l$n[v]<-ldamat[which(ldamat[,1]==v)][1:20]
for (v in seq(1:k)) {
l$n[v]<-ldamat[which(ldamat[,1]==v)][1:20]
}
l<-data.frame(`-1`=seq(1:20))
n<-seq(1:k)
for (v in seq(1:k)) {
l$n[v]<-ldamat[which(ldamat[,1]==v)][1:20]
}
l
n<-as.char(seq(1:k))
n<-as.character(seq(1:k))
for (v in seq(1:k)) {
l$n[v]<-ldamat[which(ldamat[,1]==v)][1:20]
}
l
which(ldamat[,1]==v)[1:20]
which(ldamat[,1]==1)[1:20]
which(ldamat[,1]==2)[1:20]
ldamat[10]
ldamat[,10]
ldamat
which(ldamat[,1]==2)[1:20]
ldamat[30]
etdm_topics <- tidy(topics(etdm_lda),matrix="beta")
etdm_topics <- tidy(etdm_lda,matrix="beta")
etdm_topics[100:150,]
etdm_topics <- tidy(ldamat,matrix="beta")
#### SENTIMENT ANALYSIS ####
attributes(ecleaned)
df <- data.frame(text = get("content", ecleaned),stringsAsFactors = F)
head(df$text)
sentiment <- get_nrc_sentiment(df$text)
changeWords<- function(x) gsub('australi','australia',x)
ecleaned<-tm_map(ecorpus,content_transformer(changeWords))
ecleaned<-tm_map(ecleaned,stripWhitespace)
df <- data.frame(text = get("content", ecleaned),stringsAsFactors = F)
sentiment <- get_nrc_sentiment(df$text)
head(sentiment)
head(df$text)
inspect(ecleaned[1:5])
ecorpus<-tm_map(ecorpus,tolower) #make it all lowercase
ecorpus<-tm_map(ecorpus,removePunctuation)
ecorpus<-tm_map(ecorpus,removeNumbers)
ecleaned<-tm_map(ecorpus,removeWords,stopwords("english"))
ecleaned<-tm_map(ecorpus,content_transformer(changeWords))
#remove united and airlines
ecleaned<-tm_map(ecleaned,removeWords,c("united","airlines","flight","airline","passenger","flights","will","according","…","—","’s","says","said"))
ecleaned<-tm_map(ecleaned,stripWhitespace)
inspect(ecleaned[1:5])
stopwords("english")
ecleaned<-tm_map(ecorpus,removeWords,stopwords("english"))
inspect(ecleaned[1:5])
ecleaned<-tm_map(ecorpus,content_transformer(changeWords))
inspect(ecleaned[1:5])
ecleaned<-tm_map(ecorpus,removeWords,stopwords("english"))
inspect(ecleaned[1:5])
#remove united and airlines
ecleaned<-tm_map(ecleaned,removeWords,c("united","airlines","flight","airline","passenger","flights","will","according","…","—","’s","says","said"))
inspect(ecleaned[1:5])
#remove united and airlines
ecleaned<-tm_map(ecleaned,removeWords,c('plane',"united","airlines","flight","airline","passenger","flights","will","according","…","—","’s","says","said"))
inspect(ecleaned[1:5])
changeWords<- function(x) gsub('australi','',x)
ecleaned<-tm_map(ecorpus,content_transformer(changeWords))
inspect(ecleaned[1:5])
inspect(ecleaned[1:5])
ecleaned<-tm_map(ecleaned,stripWhitespace)
df <- data.frame(text = get("content", ecleaned),stringsAsFactors = F)
sentiment <- get_nrc_sentiment(df$text)
head(sentiment)
head(df$text)
